{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy, Gini & Infromation-Gain\n",
    "- Entropy: Measurement of the randomness in the data\n",
    "- Gini: Measurement of the purity of the data\n",
    "- Information-Gain: Measurement of the reduction in entropy after splitting the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics behind the Entropy, Gini & Infromation-Gain\n",
    "`Entropy, Gini, and Information Gain` are three fundamental concepts in decision tree learning. They are used\n",
    "to evaluate the quality of a split in a decision tree. Here's a brief overview of each concept\n",
    "### Entropy\n",
    "- `Entropy` is a measure of the amount of uncertainty or randomness in a dataset. It is calculated as follows\n",
    "- `H(S) = -\\sum_{i=1}^{n} p_i \\log`\n",
    "where $S$ is the dataset, $n$ is the number of classes, and $p\n",
    "is the proportion of instances in class $i$. The entropy is a measure of how uncertain we are\n",
    "about the class label of an instance. A high entropy indicates that we are very uncertain about the class\n",
    "label, while a low entropy indicates that we are very certain about the class label.\n",
    "### Gini\n",
    "- `Gini` is a measure of the impurity of a dataset. It is calculated as follows\n",
    "- `Gini(S) = 1 - \\sum_{i=1}^{n} p`\n",
    "where $S$ is the dataset, $n$ is the number of classes, and $p\n",
    "is the proportion of instances in class $i$. The Gini index is a measure of how imp\n",
    "the dataset is. A high Gini index indicates that the dataset is very imp\n",
    ", while a low Gini index indicates that the dataset is very pure.\n",
    "### Information Gain\n",
    "- `Information Gain` is a measure of the reduction in entropy after splitting a dataset. It is calculated as follows\n",
    "- `IG(S, A) = H(S) - H(S|A)`\n",
    "where $S$ is the dataset, $A$ is the attribute to split on, and $\n",
    "is the entropy of the dataset after splitting on attribute $A$. \n",
    "- The Information Gain is a measure of\n",
    "how much the entropy of the dataset is reduced after splitting on attribute $A$. A high Information Gain\n",
    "indicates that the attribute $A$ is a good split, while a low Information Gain indicates that\n",
    "attribute $A$ is not a good split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import the Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the library of math\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a example dataset:\n",
    "n_A = 4\n",
    "n_B = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum of all:\n",
    "total = n_A + n_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of A is:  0.4\n",
      "The proportion of B is:  0.6\n"
     ]
    }
   ],
   "source": [
    "#Lets Calculate the Proportion:\n",
    "p_A = n_A/total\n",
    "p_B = n_B/total\n",
    "print(f\"The proportion of A is: \", p_A)\n",
    "print(f\"The proportion of B is: \", p_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the entropy on dataset:\n",
    "entropy = - p_A * math.log2(p_A) - p_B * math.log2(p_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy:  0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "#Print Entropy:\n",
    "print(\"Entropy: \", entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the gini impurity:\n",
    "gini_impurity = 1-p_A**2 - p_B**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Impurity:  0.48\n"
     ]
    }
   ],
   "source": [
    "#Print the gini impurity:\n",
    "print(\"Gini Impurity: \", gini_impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assume a data divided into the subgroups for information gain:\n",
    "n_1_A, n_1_B = 1, 3\n",
    "n_2_A, n_2_B = 2, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total the subgroups:\n",
    "total_1 = n_1_A + n_1_B\n",
    "total_2 = n_2_A + n_2_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divided into the subproportions:\n",
    "p_1_A = n_1_A / total_1\n",
    "p_1_B = n_1_B / total_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the entropy:\n",
    "entropy_1 = - p_1_A*math.log2(p_1_A) - (p_1_B*math.log2(p_1_B) if p_1_B else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the p_2_A, p_2_B:\n",
    "p_2_A = n_2_A / total_2\n",
    "p_2_B = n_2_B / total_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the entropy_2:\n",
    "entropy_2 = - (p_2_A*math.log2(p_2_A)) - (p_2_B*math.log2(p_2_B) if p_2_B else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain:  0.09546184423832171\n"
     ]
    }
   ],
   "source": [
    "#Calculate the information gain:\n",
    "info_gain = (entropy) - ((total_1)/total*entropy_1 + (total_2)/total*entropy_2) \n",
    "#Print the total info_gain:\n",
    "print(\"Information Gain: \", info_gain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonl_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
